# pmm
首先说pmm模块的管理，pmm管理说到底就是管理页框
这部分，首先需要从multiboot获取所有可用物理内存的位置，并且减去内核占用调的部分。

随后我打算分成多个块，使用一个链表串联起来这些块（由于本身存在对齐的问题，并且linux也是分块进行内存分配的）

另外，需要使用buddy算法来分配物理页框，管理具体某一块的内容。

# buddy实现
首先参考Linux的实现中，他将内存划分为几个部分，称为zone
每个部分有个水位线
记住：考虑到后续的一些内存置换的需求，还会增加数据结构，所以尽量用sizeof(page_frame)去做
然后每个部分在这里面buddy会分桶，每个桶是2的倍数，针对不同的桶，去做一个链表。
在这里的设计，我们直接简单粗暴的对每个桶使用一个大的page_frame数组，分别表示每个桶里面的所有page。
事实上，静态开销相比之下更大一些。但是因为是数组，不需要动态分配，可以使用，也有其优势。

对于alloc
最开始应该是只有最高阶的桶会存在链表，如果要分配1个page，那么会从下向上查找，找到链表头的一个page_frame，先分成两部分，对应的page_frame结构是非常好查找的。把空的那块插入链表，不空的那块继续分配。

分配完了之后，如果这个页面已经有被分配了的，需要置位have alloc。

如果一整个大页面都分配了，需要递归的置位。

对于free，其实应该设计两个函数，第一个是free单个page，第二个是free多个page

对于单个page，首先检查是否分配出去了，如果没分配出去，那滚蛋，错误的free

然后释放，挂链表，如果兄弟也释放了，那么摘下兄弟，并把父亲给挂上。

对于多个page，他未必是按照order的内容去分配的，但是必须是连续的。
举个简单的例子

两层

 [0,1]   [2,3]   [4,5]
[0] [1] [2] [3] [4] [5]

全都是满的
现在要释放1，2，3，4，四个页面。
这里面，从1开始，1最后一位是1，所以他只能释放1个页面
然后现在最低变为2，2小于等于4，所以他最多只能释放2个页面，所以需要把 [2,3] 和 [2] [3] 都释放掉。
然后是4，b100，所以最多可以释放4个页面，但是，因为4+4> 4，所以只能到最终端，就是4,4的区间释放掉4，只有一个页面。
如果有多层的话，2向上溯只能上溯一层，但是4可以向上溯2层，如果判断出来需要释放4,8，那还需要继续上溯。
直到顶层页面。

相比于Linux的bitmap的异或，这未必是最优的，但是是可行的，而且实现难度更低一些。


# pmm内存的layout

high address
	-- - -- - -- - -- -
	buddy static part
	-- - -- - -- - -- - _end ROUND_UP 4K
	empty
	-- - -- - -- - -- -	_end
	kernel
	-- - -- - -- - -- - 0x100000
	bios part	:we cannot use
	-- - -- - -- - -- - 0x0
low address

# 这种静态buddy算法能够支持的物理空间（以下在x86下）
在这里，我们会设定好，buddy静态数据区和kernel必须是连在一起的
实际上，在qemu中，一般内存开不了一块这么大的连续内存，我实测出来，一块连续的内存，最大为len=0x7fedf000
估算一下大概是2G不到一些（2046M左右）。
考虑我们的算法，
物理地址的空间目前在qemu中为40bits
每4k被压缩成为一个u64，也就是28位的ppn，每个ppn需要占用8Byte。
由于类似于一棵树，所以层级存储还需要平均额外占用8Bytes（不到）

所以ppn需要约16Byte，
即需要28+4位，也就是刚刚好32位的空间，用于存储最大1024G的40bit物理内存空间。
实际上，按照这么算，这个算法在qemu下需要4G的空间用于记录40bit的最大物理内存空间。
尽管实际上，我估计这个内核大概率不会有机会在这么大内存的机器上运行，不过我必须记录下这个问题。
这个问题的实质在于，我把buddy空间直接放在kernel空间之后了，而事实上，这两部分可以分开，找到一个足够大小的空间用于存放。
(#见下文设计bug修复，此问题已经在考虑中了。)

同样更为复杂的是，我需要使用32位来表示这么多的page，但是实际上，我只能表示1G的空间，因为对于ppn，我使用的仅仅是30位(见设计bug修复，现在是28位了)。所以事实上，我大概能使用不到256G左右的空间。尽管对于正常普通pc来说是足够了，但是显然他不是一个足够好的算法，因为在应对足够大内存（现在大于1T的内存也很正常了）的时候，会存在问题。
所以需要加一个检查，查看是否大于1G。

设计bug修复下的方案：我的entry只记录index，所以28位即可完成压缩，这样2位用于表示是否使用，6位用于shared memory的存放（可以记录64，应该是足够的）

一个简单且也许可行的方法是，类似于Linux打补丁一样，对于过大的内存，设计类似于High mem这样的东西去做。
但是总之，起码既然现在他还能应对，我就不多做尝试了。

# shared memory 的处理。
在一开始我的确试图让这个进入page这个struct，但是，很显然的一个事情是，真正用于实用的shared memory是并不多的，相比于used而言。

而由于对齐的原因，如果我选择增加8位的数据，实质上他会增加32bit，为此增加一个字段的代价，在我上述的分析中，这是一个巨大的开销（显然）。即便不考虑这些，不对齐带来的访存影响说不定更大（未经实证）。

总而言之，我拒绝为这个page记录增加shared访问次数这个字段，不过，我还是需要增加一个shared mem位，用于表示是否检查引用计数。
并且，需要为此增加一个数据结构用于存放这些用于shared的ppn
而在释放的时候，需要额外考虑这些。

# 设计bug修复
在之前的代码设计中，还有一个问题，它所有的内容都放在物理地址1G以下，但是，很不巧的是，ARM的qemu的起始地址就是1G的位置。
另外，所有的bucket都放在最低的物理地址上的设计，也带来不够灵活的问题。所以增加如下设计

给每个bucket增加一个bucket的物理 phy的基地址。
next的index则只是单纯的表示在当前这个array中的偏移。

从而，pmm_buddy不必强制要求放在最低地址。

# zone的avaliable list的问题

在原有的设计下，他仍然可以正常工作，但是在新的设计下，所有的index都被有需要的时候用于表示物理内存了，所以，因此，我只能选择不使用头结点的方式

在这种方式下，在插入的时候需要判断这个指针是否为空，如果为空直接赋值avaliable_zone_head[index]，否则则调用frame list add 系列函数

同时，在删除的时候，需要判断是否只剩下这一个节点了，如果是，则将avaliable_zone_head[index]置为null，否则则正常调用frame list del