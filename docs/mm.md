# pmm
首先说pmm模块的管理，pmm管理说到底就是管理页框
这部分，首先需要从multiboot获取所有可用物理内存的位置，并且减去内核占用调的部分。

随后我打算分成多个块，使用一个链表串联起来这些块（由于本身存在对齐的问题，并且linux也是分块进行内存分配的）

另外，需要使用buddy算法来分配物理页框，管理具体某一块的内容。

# buddy实现
首先参考Linux的实现中，他将内存划分为几个部分，称为zone
每个部分有个水位线
然后每个部分在这里面buddy会分桶，每个桶是2的倍数，针对不同的桶，去做一个链表。
在这里的设计，我们直接简单粗暴的对每个桶使用一个大的page_frame数组，分别表示每个桶里面的所有page。
事实上，静态开销相比之下更大一些。但是因为是数组，不需要动态分配，可以使用，也有其优势。

对于alloc
最开始应该是只有最高阶的桶会存在链表，如果要分配1个page，那么会从下向上查找，找到链表头的一个page_frame，先分成两部分，对应的page_frame结构是非常好查找的。把空的那块插入链表，不空的那块继续分配。

分配完了之后，如果这个页面已经有被分配了的，需要置位have alloc。

如果一整个大页面都分配了，需要递归的置位。

对于free，其实应该设计两个函数，第一个是free单个page，第二个是free多个page

对于单个page，首先检查是否分配出去了，如果没分配出去，那滚蛋，错误的free

然后释放，挂链表，如果兄弟也释放了，那么摘下兄弟，并把父亲给挂上。

对于多个page，他未必是按照order的内容去分配的，但是必须是连续的。
举个简单的例子

两层

 [0,1]   [2,3]   [4,5]
[0] [1] [2] [3] [4] [5]

全都是满的
现在要释放1，2，3，4，四个页面。
这里面，从1开始，1最后一位是1，所以他只能释放1个页面
然后现在最低变为2，2小于等于4，所以他最多只能释放2个页面，所以需要把 [2,3] 和 [2] [3] 都释放掉。
然后是4，b100，所以最多可以释放4个页面，但是，因为4+4> 4，所以只能到最终端，就是4,4的区间释放掉4，只有一个页面。
如果有多层的话，2向上溯只能上溯一层，但是4可以向上溯2层，如果判断出来需要释放4,8，那还需要继续上溯。
直到顶层页面。

相比于Linux的bitmap的异或，这未必是最优的，但是是可行的，而且实现难度更低一些。

在整个buddy pmm结构体中，使用多个bucket数组表示所有的内存之后

考虑zone，每个zone当中，主要使用两个指针数组来表示，一个指针数组用于表示在不同的order中，这个zone所指向的第一个page frame，在这个指针之前的page frame则是属于前一个zone，在这个frame地址之后的所有frame直到下一个zone的第一个frame，包括其中的可用的不可用的frame。

另一个指针数组则是用于描述可用的链表。这些frame则不是连续的，而是使用链表来连接的，也就是上面说的那个buddy的数组了。

# pmm内存的layout(x86_64)

high address
	-- - -- - -- - -- -
	buddy static part
	-- - -- - -- - -- - _end ROUND_UP 4K
	empty
	-- - -- - -- - -- -	_end
	kernel
	-- - -- - -- - -- - 0x100000
	bios part	:we cannot use
	-- - -- - -- - -- - 0x0
low address

# 这种静态buddy算法能够支持的物理空间
在这里，我们允许每个bucket的位置都可以分离而不是所有的bucket都必须连续的进行分配
除此之外，原先我的设计，将内核放置在最后1G的空间中。
但是考虑到极大的内存分配过程。这点空间是不够的，
另外，原先这是一个todo，但是现在被废弃了，大概是这样的，考虑到系统可能使用多个memory区间，然后需要存放这些page frame的情况，如果内存需要占用的空间非常大，pageframe 的一对array并不总是能够放到跟内核连续的这个region里面
一个简单的想法是，我换个区间去放，另外，我多个pageframe的array不一定需要连续的存放。
但是，我放弃了这种修改，原因是它需要不定的去增加新的1G页，这也极大的增加了我的工作量。
目前，我的pageframe必须存放在和kernel相同的1G空间内。

如果不考虑这种限制。
此时，对于能够支持的物理空间的限制，仅仅只有在page_frame中的next和prev的位数的限制，我这边设置28位是因为在qemu的x86_64中使用了40位的物理内存空间（大概1T）

因此为了尽可能的通用，在aarch64的设计中，我也将tcr_el1的ips位设为最大40，防止出事情。

# shared memory 的处理。
对于每个page frame，还设置了一个字段表示shared memory的count数量，我认为6位表示count==64是完全足够的
不过，现在并没有加上这部分的处理逻辑，因此，这部分留作TODO


# zone的avaliable list的问题

在原有的设计下，他仍然可以正常工作，但是在新的设计下，所有的index都被有需要的时候用于表示物理内存了，所以，因此，我只能选择不使用头结点的方式

在这种方式下，在插入的时候需要判断这个指针是否为空，如果为空直接赋值avaliable_zone_head[index]，否则则调用frame list add 系列函数

同时，在删除的时候，需要判断是否只剩下这一个节点了，如果是，则将avaliable_zone_head[index]置为null，否则则正常调用frame list del

不仅仅如此，我在实践过程中发现，还需要在del head的时候更新链表的头节点指针，以及在add head的时候也需要更新（注意更新头节点指针和做删除以及add的顺序）

# 虚拟内存的处理
最开始我以为需要vma结构体进行管理，并实现了红黑树，但是，后来我意识到，只有在用户使用的空间中，需要进行如此管理，而在内核使用的的空间中，则无需如此。这是个恒等映射即可。

所以实际上，需要向pmm要一块空间，并随即映射到高地址区域。通过slub进行管理即可，虽然其实也可以用红黑树就是了。
对于vma管理结构，虽然很经典，也是我经常遇到的东西，不过，我觉得我在之前缺少实现的经验，所以会错误的把内核这部分也用vma进行管理。


# 内存分配器
首先是描述如何从内核获取页面
我们使用恒等映射的方式去做，所以，当我们从pmm获取一个或者若干个页面的时候，直接映射过去即可。
除此之外，正如上面说的那样，这部分内核的管理不能依赖于vma的管理，而是让内核内存分配器来完成
关于内存分配器的管理。我决定自己设计（实在是那个pmm的page结构，没法拿来复用。。。比如slob就直接用union‘寄生’到这个page结构体中，但我不能这样做）
至于slub，无论如何还是过于复杂了。

对于实现的设计如下
实现为按照8/16/24/32/48/64/96/128/256/512/1024/2048这几个大小

slub的cpu和numa node两层对于os级是可以考虑的。（说到底，我只有一个cpu和一个numa node，不过为了后续可扩展性，这部分设计需要预留出来，cpu将来大概率会扩展成多核）
对于cpu的list的访问，可以不加锁
而对于numa node需要加锁访问。

如果上述都失败了，才会去找pmm继续分配，然后映射过去。
