# pmm
首先说pmm模块的管理，pmm管理说到底就是管理页框
这部分，首先需要从multiboot获取所有可用物理内存的位置，并且减去内核占用调的部分。

随后我打算分成多个块，使用一个链表串联起来这些块（由于本身存在对齐的问题，并且linux也是分块进行内存分配的）

另外，需要使用buddy算法来分配物理页框，管理具体某一块的内容。

# buddy实现
首先参考Linux的实现中，他将内存划分为几个部分，称为zone
每个部分有个水位线
然后每个部分在这里面buddy会分桶，每个桶是2的倍数，针对不同的桶，去做一个链表。
在这里的设计，我们直接简单粗暴的对每个桶使用一个大的page_frame数组，分别表示每个桶里面的所有page。
事实上，静态开销相比之下更大一些。但是因为是数组，不需要动态分配，可以使用，也有其优势。

对于alloc
最开始应该是只有最高阶的桶会存在链表，如果要分配1个page，那么会从下向上查找，找到链表头的一个page_frame，先分成两部分，对应的page_frame结构是非常好查找的。把空的那块插入链表，不空的那块继续分配。

分配完了之后，如果这个页面已经有被分配了的，需要置位have alloc。

如果一整个大页面都分配了，需要递归的置位。

对于free，其实应该设计两个函数，第一个是free单个page，第二个是free多个page

对于单个page，首先检查是否分配出去了，如果没分配出去，那滚蛋，错误的free

然后释放，挂链表，如果兄弟也释放了，那么摘下兄弟，并把父亲给挂上。

对于多个page，他未必是按照order的内容去分配的，但是必须是连续的。
举个简单的例子

两层

 [0,1]   [2,3]   [4,5]
[0] [1] [2] [3] [4] [5]

全都是满的
现在要释放1，2，3，4，四个页面。
这里面，从1开始，1最后一位是1，所以他只能释放1个页面
然后现在最低变为2，2小于等于4，所以他最多只能释放2个页面，所以需要把 [2,3] 和 [2] [3] 都释放掉。
然后是4，b100，所以最多可以释放4个页面，但是，因为4+4> 4，所以只能到最终端，就是4,4的区间释放掉4，只有一个页面。
如果有多层的话，2向上溯只能上溯一层，但是4可以向上溯2层，如果判断出来需要释放4,8，那还需要继续上溯。
直到顶层页面。

相比于Linux的bitmap的异或，这未必是最优的，但是是可行的，而且实现难度更低一些。

在整个buddy pmm结构体中，使用多个bucket数组表示所有的内存之后

考虑zone，每个zone当中，主要使用两个指针数组来表示，一个指针数组用于表示在不同的order中，这个zone所指向的第一个page frame，在这个指针之前的page frame则是属于前一个zone，在这个frame地址之后的所有frame直到下一个zone的第一个frame，包括其中的可用的不可用的frame。

另一个指针数组则是用于描述可用的链表。这些frame则不是连续的，而是使用链表来连接的，也就是上面说的那个buddy的数组了。

# pmm内存的layout(x86_64)

high address
	-- - -- - -- - -- -
	buddy static part
	-- - -- - -- - -- -	per cpu part end
	per_cpu data
	-- - -- - -- - -- -	_end
	kernel
	-- - -- - -- - -- - 0x100000
	bios part	:we cannot use
	-- - -- - -- - -- - 0x0
low address

# pmm内存的layout(aarch64)

high address
	-- - -- - -- - -- -
	buddy static part
	-- - -- - -- - -- -	per cpu part end
	per_cpu data
	-- - -- - -- - -- - _end ROUND_UP 2M+6M
	dtb map(for dtb at most have 2M, but might not aligned ,so we should alloc 4M)
	-- - -- - -- - -- - _end ROUND_UP 2M+2M
	empty
	-- - -- - -- - -- - _end ROUND_UP 2M+4K
	pl011
	-- - -- - -- - -- - _end ROUND_UP 2M
	-- - -- - -- - -- -	_end
	kernel
	-- - -- - -- - -- - 0x40080000
	-- - -- - -- - -- - 0x40000000
	hardware part	:we cannot use
	-- - -- - -- - -- - 0x0
low address

here we define the per_cpu part plus buddy as extra part

# 这种静态buddy算法能够支持的物理空间
在这里，我们允许每个bucket的位置都可以分离而不是所有的bucket都必须连续的进行分配
除此之外，原先我的设计，将内核放置在最后1G的空间中。
但是考虑到极大的内存分配过程。这点空间是不够的，
另外，原先这是一个todo，但是现在被废弃了，大概是这样的，考虑到系统可能使用多个memory区间，然后需要存放这些page frame的情况，如果内存需要占用的空间非常大，pageframe 的一对array并不总是能够放到跟内核连续的这个region里面
一个简单的想法是，我换个区间去放，另外，我多个pageframe的array不一定需要连续的存放。
但是，我放弃了这种修改，原因是它需要不定的去增加新的1G页，这也极大的增加了我的工作量。
目前，我的pageframe必须存放在和kernel相同的1G空间内。

如果不考虑这种限制。
此时，对于能够支持的物理空间的限制，仅仅只有在page_frame中的next和prev的位数的限制，我这边设置28位是因为在qemu的x86_64中使用了40位的物理内存空间（大概1T）

因此为了尽可能的通用，在aarch64的设计中，我也将tcr_el1的ips位设为最大40，防止出事情。

# shared memory 的处理。
对于每个page frame，还设置了一个字段表示shared memory的count数量，我认为6位表示count==64是完全足够的
不过，现在并没有加上这部分的处理逻辑，因此，这部分留作TODO


# zone的avaliable list的问题

在原有的设计下，他仍然可以正常工作，但是在新的设计下，所有的index都被有需要的时候用于表示物理内存了，所以，因此，我只能选择不使用头结点的方式

在这种方式下，在插入的时候需要判断这个指针是否为空，如果为空直接赋值avaliable_zone_head[index]，否则则调用frame list add 系列函数

同时，在删除的时候，需要判断是否只剩下这一个节点了，如果是，则将avaliable_zone_head[index]置为null，否则则正常调用frame list del

不仅仅如此，我在实践过程中发现，还需要在del head的时候更新链表的头节点指针，以及在add head的时候也需要更新（注意更新头节点指针和做删除以及add的顺序）


## 物理内存分配器的改版设计
### 改写 step1
在原始的设计中，我尽可能地压缩了需要的内存
```
struct page_frame {
        u64 flags : 2;
        u64 shared_count : 6;
        u64 prev : 28;
        u64 next : 28;
} __attribute__((packed));
```
从节约内存的角度来说，这样确实不错，不过不代表这样是合理的，节约内存的代价是，我需要将prev和next通过一些复杂的地址转换，变为对应的数据结构，而这在代码实现上并不够直观。

所以让我给出新的buddy实现吧
```
struct page_frame {
		u32 index;
		u32 flags;
		u64 ref_count;
		struct page_frame* prev;
		struct page_frame* next;
};
```
在这么改写之后，我们就能够使用一个struct list_entry去链接整个物理地址空间。
当然，一口气吃不成一个胖子，从设计上来说，还有更多的设计，比如切分为多个链表等等操作。不过，我们第一步，只需要改写page_frame，并给出新的结构体定义即可，其他的，如根据zone划分为多个链表的工作，可以之后再做。

这个改写之后，目前所有的alloc和释放，都已经通过zone内部的数据结构来进行分配和释放了。
另外，还有性能上的问题。当然，按我的理解，这是需要改造成按照zone进行分配释放的必要的一个步骤，这个省不掉，那也没辙了。

### 改写 step2
但是，按下葫芦浮起瓢，另一个问题立马就浮现出来，这里的page frame占用了4个u64,那，我还真有96G内存的平台。这导致，它有个判定过不去，因为pmm这些page frame的地址跨越了1G范围。这就很坏了。

归根结底的问题是，我没有考虑动态的调整，因此还需要做额外的设计来防止这个问题。我的设计如下，在percpu之后，pmm之前，放入这部分的页面。

然后在上一层的L1 table的地方，去动态地增加这部分的页面，再去遍历所有的新的L2 table，去做映射。

在实际代码实现中，也要进一步的进行步骤的划分。

第一步，当然是将这部分额外的范围考虑进来，增加到pmm size的区域中（空间得算对）

第二步骤，是先做第一个L2 table的映射。这些被映射的连续4K区域，实际上会被当作L1_table的孩子，我相信，正常来说，1个L2 table映射的范围是足够的（每个entry是2m,所有的是1G，这些1G空间当成L1 table的孩子，最多可以管理512G空间，而这512G空间全都用于存放page_frame，这可以管理的空间绝对够用了。当然实际上，这个需要1G减去直到percpu这一段的空间，percpu如果变量太多，core太多，也会有问题。不过，总而言之，不算少了）

第三步骤，如果发生了超过1G范围的情况，这就需要L1_table按照实际的数量，逐个映射这些连续的4K区域（这是个自映射哦），作为额外的L2_table

第四个步骤，在这些额外的L2_table中，再去映射需要的pmm的page frame结构体。从而满足需要的条件。

（当然，这需要注意边界条件）
### 改写 step3
在这个改写之后，更大的问题又来了。主要来源于x86平台，x86平台的4G范围（32bit的地址空间）是一个障碍，他会在地址空间末尾放一些东西，所以实际上会导致在这个区域放不下东西的问题，这时候需要动态挪动这些page frame存放的位置（但是已经需要200G内存了）。

经过上述更改之后，看上去是可以了，但是新的问题又来了。在linux中，存在DMA zone（我们假定现代设备都有iommu或者smmu，那这个空间可以不用），或者CMA zone，这个区域。显然CMA zone这个zone需要的空间并不是使用buddy来管理的。

所以需要做进一步的抽象，也就是说，zone是一个高于buddy的概念。整个物理内存管理器，也可能包含多个使用不同内存管理方案的zone。

这意味着要从原先的buddy_pmm进一步的去向上抽象，同时改写不同arch的pmm init的代码。

在percpu空间处理之后，需要分配各个zone的区域，分配zone算法和calculate_avaliable_phy_addr_end需要提升到整个系统的pmm中

而calculate_pmm_space，不同的分配器会有不同的实现。

这个也需要考虑buddy pmm特有的，和全局的计算额外l2 table需要的空间的，所有的pmm空间都被放在一起（或者链接到一起？）可以考虑

generate_pmm_data则应当是每个分配器特有的。

另外需要额外注意那个pmm free的东西，他需要通过这个指针来查找一个zone，这个也应该放到系统的pmm的管理中去，而不应该放在buddy中，只有指定了是一个buddy，才去调用buddy的释放。

从而进一步的剥离buddy物理内存分配算法和具体pmm zone的物理内存分配器的关系。

最终实现这样的效果：整个物理内存管理分为多个zone，每个zone都有一个buddy或者其他的分配器，按照实际的申请的zone的number（或者实际上可能基于zone的flag来进行进一步的策略管理）

另外一个问题是，物理内存管理数据结构，目前的分配方法是，per cpu region之后最开始有一个L2table，在这之后是pmm管理的数据结构，不同的zone全都放在一起，它进行连续的分配（如果有DMA zone，应当放在percpu之前，因为，percpu还是比较花费空间的，只有内核那不多）。

当然，这也需要拆分。

同样的，不能一口气吃成胖子，先改造整个数据结构，再说后面的不同区域的pmm管理结构拆分的问题。所以，我们需要简单的为dma zone（还是设置一下吧）设置一个zone，在aarch64下，也选择对应的一块1g开始的16m的空间。然后使用简单的线性分配器来实现该zone的分配释放。以此作为改造的标准，当然，可能使用pmm的接口调用也得重新设置一下。

# 虚拟内存的处理
最开始我以为需要vma结构体进行管理，并实现了红黑树，但是，后来我意识到，只有在用户使用的空间中，需要进行如此管理，而在内核使用的的空间中，则无需如此。这是个恒等映射即可。

所以实际上，需要向pmm要一块空间，并随即映射到高地址区域。通过slub进行管理即可，虽然其实也可以用红黑树就是了。
对于vma管理结构，虽然很经典，也是我经常遇到的东西，不过，我觉得我在之前缺少实现的经验，所以会错误的把内核这部分也用vma进行管理。


# 内存分配器
首先是描述如何从内核获取页面
我们使用恒等映射的方式去做，所以，当我们从pmm获取一个或者若干个页面的时候，直接映射过去即可。
除此之外，正如上面说的那样，这部分内核的管理不能依赖于vma的管理，而是让内核内存分配器来完成
关于内存分配器的管理。我决定自己设计（实在是那个pmm的page结构，没法拿来复用。。。比如slob就直接用union‘寄生’到这个page结构体中，但我不能这样做）
至于slub，无论如何还是过于复杂了。

对于实现的设计如下
实现为按照8/16/24/32/48/64/96/128/256/512/1024/2048这几个大小

slub的cpu和numa node两层对于os级是可以考虑的。（说到底，我只有一个cpu和一个numa node，不过为了后续可扩展性，这部分设计需要预留出来，cpu将来大概率会扩展成多核）
对于cpu的list的访问，可以不加锁
而对于numa node需要加锁访问。

如果上述都失败了，才会去找pmm继续分配，然后映射过去。

## 实现

在实际中，我使用了如下的实现

从上到下，最上层
若干个allocator，他们依赖于一个get_free_page模块，互斥的从内核获取直接映射的块
page的管理模块

每个allocator下面包含12个group，每个组里面的object payload的大小是8/16/24/32/48/64/96/128/256/512/1024/2048

每个group里面包含两个链表
一个是部分使用的链表，一个是空链表
空链表的作用是，如果其他group不够用了，那么优先从这些group的空链表上供给
只有其他组的空链表都不够用了，才统一从系统为每个group的空链表分配一次chunk

每个链表由若干个chunk串联起来，一个chunk被设计为包含2个page。
在chunk头部有一个头，然后后面跟一定大小的padding，之后密集排列objects

object内部有个header，以及alloc实际返回的addr的payload。
所以每个object实际的大小分别为
24/32/40/48/64/80/112/144/272/528/1040/2064
之所以这么设计
1、统一是8192，所以free的时候根据地址很快的发现位于chunk头部的header管理结构
2、因为包含了头部以及每个object的头部，所以对于2048大小的块要紧密排列是困难的。如果只有一个page，那么浪费太严重了。
3、还必须考虑释放的问题，如果一个chunk里面有非常多的pages，那么很可能哪怕这个块只有1个使用了的object，仍然难以释放，chunk占用的page越多，这种浪费越严重。
所以综合考虑，我认为一个chunk 2page是合适的。


除此之外还需要考虑的是分配和释放的时候，对于一整页的处理
因为在分配的时候，如果是4096的整数倍，我们会试图直接调用get free page去做。在内核中，需要非常大的缓冲buffer的时候，是可能发生的。但是通常来说，这不应该超过2M（否则buddy难以处理）

这种类型的指针非常好识别，他一定是4K对齐的，而在allocator内部，在第一个page中，4K对齐的地址是header的地方，而第二个page中，可以计算出来，上述的object payload也不会对齐到4K的位置。

所以，可以通过这种方式非常简单的判定，是不是整个页面进行的分配。
但是如果试图释放，我们需要知道多少个page在里面。
同样的道理，对于allocator自己本身使用的page，也需要管理以便于释放

因此，还需要在所有allocator之外，增加一个所有page的虚拟页的管理器。
这个管理器我决定使用红黑树来实现。

===


具体的实现是这样的
我试图让他bootstrap

然后结构上，每个节点包括一个红黑树节点，一个free list，起始地址和size，size应该是表示分配的页面号的，使用u32表示size即可，如果表示终止地址需要u64，浪费
最后多一个管理管理节点页面相关的变量，u32，表示这个管理节点所管理的页面是存储管理节点的，并且其值表示该页面剩余的可用管理节点数目，用于等于零的时候去释放该管理页面。

用于管理的节点，以一个array的方式，排列在一个内存页面中

当试图申请一个页面的时候，首先检查申请的页面数量，超过2M，就不用分配了（我们默认内核不会干这种事情，内核的数据结构不应该这么大）
这里有一个manage free list，在这个链表中，存放了有空闲entry的管理页面的链接，在释放的时候，会通过红黑树去找，如果释放之后，查看这个manage free list node，发现指向自己，说明他现在不在整个free list中（因为头在根节点中），此时他释放了一个entry，需要通过根节点的header，插入。
如果插入之后，发现整个链表用完了所有的空间，就从这个free list上摘出去。
初始化的时候，需要先初始化根节点，再把它放到根节点指向的链上去。

查看free list是否可以分配一个管理节点
如果可以分配，那么从free list上分配该管理节点，申请相应的物理页帧并恒等映射，同时修改管理节点的起始地址和结束地址
找到该页面的管理节点（也就是该页面的第一个节点），将其剩余可用管理节点数量减一。
同时插入红黑树

如果free list不能分配一个节点，说明他已经在使用的所有管理页面中没有足够的空间了，需要新分配一个管理页面。这时候，可以先查看后备管理页面
然后，这时候，从页帧分配器中要一个页面并恒等映射。将该页面进行分割，第一个节点被当做该管理页面的管理节点（也就是自举）
执行插入红黑树，同时指定起始地址和size。
同时，对其进行初始化，将剩下的(pagesize-管理节点的size)的空间进行分割成一个个的管理节点，并插入free list链表（即初始化该空管理页）

当试图释放一个页面（或者连续的若干个页面）的时候，从红黑树中查找该页面，找到该页面的管理节点。根据该管理节点，释放该页面给pmm，并取消映射
同时将该节点从红黑树中删除，并插入free list，该管理节点的起始地址和size进行清理。该页面的管理节点的可用管理节点数目减一。

当减为0的时候，查看根节点（一会描述）是否包含有后备页面指针，如果没有，存入后备管理页面，防止抖动问题（也就是这里刚释放完，立马试图再分配一个，反复释放分配，会造成抖动。）如果存在后备管理页面，则释放本管理页面。

根节点（需要一个根节点来描述整个红黑树，这是由于红黑树本身结构导致的），为了自举，我们保证这个根节点管理结构和管理节点相比，内存占用不得大于管理节点。从而可以替换掉他（就是用个union，完事）
所以在初始化的时候，我们分配一个管理页面，0号节点用于放置管理节点，然后1号节点实质上是个根节点。
将根节点的指针指向0号节点的指针
同时根节点有个指向后备页面指针的东西，防止抖动。
还需要有个指向pmm的指针。

注意！！！

我们试图从pmm进行分配，pmm其实本身包含很多的zone。这里面需要加锁。TODO

pmm不仅仅只用于内核的数据结构，还用在比如说用户的内存的分配（实际上是page fault处理的），所以，请务必分清楚哪些代码是pmm需要运行的，哪些代码是内核数据结构的。

同时，我们需要提供get_free_page和free_pages的接口，这里面参数需要指定使用哪一个zone，等等细节。总而言之，我认为我描述清楚了这个页面管理的算法。

对于名称，我决定取个名字叫Nexus，嗯，就叫这个了。

# 页表的管理
对于页表的处理，我同样遇到了先有鸡还是先有蛋的问题
只考虑最简单的恒等映射的情况
当我试图从页帧分配器拿一个新的页面的时候，可能需要创建新的页表中的页面（可能存在于多级页表中都触发这种创建）

然后这时候，我需要继续的为了这个页面去创建新的页表映射。而一个设计不合理的页表映射到虚拟地址的方案，可能会造成多次的这种创建。
从而难以下手。

我的解决方案

首先，对于页表，我只是需要在修改他的时候能够访问这个内存，并进行修改即可，但是没说在正常转换的时候仍然需要这个内存。
因此，我有理由认为。我可以在结束后，取消这个映射。

所以每次更新页表，我只需要做最多四次这种映射，然后修改对应位即可。如果缺失，则只需要分配一个新的页，只做映射到固定的四个页面上。修改对应的位也就完事了。
也就是说，在虚拟空间中，不保留所有映射的内存，只有需要访问的时候，一遍一遍的映射即可。

我原本试图使用页表自映射机制，但是正常情况下内核不应该使用超过512GB的空间，不过，谁让我们的设计给了1T呢，需要额外考虑代码，就相对复杂，如果后期需要，可以更改。

另外，这几个用于映射的临时页面的地址，在实际中，是可以认为是整个系统运行期间都是固定的，所以可以简单的计算变成全局变量来指明对应的页表项的指针。当然，也可以不用四个页面，只用一个页面，更加节省空间。但是我认为这需要多次tlb flush,反而造成性能损失。因此不做如此设计。

关于全局变量分配这个页面还是分配在栈上面。
我倾向于全局变量，因为可以预先计算出来所有的index，而栈上的好处是减少对应的攻击可能性，相应的，每次重新计算，需要额外的计算开销。

# 总结
以上这些，我也算是一边编写代码，一边加以设计的。所以如果存在和上述描述冲突的细节，请务必以代码为准，因为代码可以跑通。
总体而言，还是主要以分层思想为主
最底下是一个buddy pmm
这个是所有的核心应当且必须公用的
在这之上，每个核心都应当有一个map handler映射器，用于辅助映射
基于这个映射器，实现了每个核心都有的nexus，包含了对于可能的虚拟页面的获取和释放。
基于nexus，再实现了一个spmalloc的一个内核对象分配器。
正常来说，在内核中，由于我尽量去获取2M的大页，其实分配器需要锁定buddy pmm的情形很少。所以实际上会留给pmm的锁竞争压力并不大。

目前还没考虑到swap的问题。正常而言，如果从buddy获取不到页面，需要进行置换。
但是这一步应当在nexus的错误处理步骤中进行处理。也就是说，如果从nexus获取不到页面，他在这里会进行swap，以供内核存在足够的页面进行操作。

# 增加的设计1
额，迫不得已，又要新加设计了。
简单来说，是关于nexus是否能够成为全局分配器的问题。
首先，对于内核态和用户态而言，这两者显然不能混为一谈。在内核态，由于集中统一直接映射，这不会存在问题。但是，在用户态，则不是这样。
因为本身记录的就是虚拟地址，而且其映射关系是混乱的，因此，可能在不同的地址空间用同一个虚拟地址。这怎么可以呢

所以，事实上，还需要额外考虑内核态和用户态的差别。
考虑到aarch64使用两个页表的情况。我觉得，这是可行的。
也就是说，所有地址空间的内核态公用一个nexus分配页面，然后每个用户态各自分配用一个nexus分配页面。
但是也正因为如此，还需要判定用户态的地址，（因为需要放在TTBR0那里）
这时候，就得根据传入地址的位置了，如果是开头17位全都是1的地址，都被认为是内核的地址（按照canonical格式，前16位是符号扩展，所以还得加一位符号位才行），使用内核的分配器。否则使用用户的分配器。
事实上，我们通常定义KERNEL_VIRT_OFFSET作为内核情况下的参数。

而内核分配器则是每个cpu一个，用户的分配器则是每个进程一个。
但是用户的分配器又有一些特殊。
在nexus中，涉及到两种类型的页面，一种是管理页面，一种才是分配出去的页面。
管理页面必须放置在内核空间中，否则可能被用户本身所破坏掉。
其他的页面都被放置在一个用户地址空间中。

为此，需要改动的为以下部分
map函数中需要增加自选的falgas
以及在get free pages以及free pages的逻辑中，增加对于用户态页面的判定。
还有就是映射页面的ppn，在用户态，并没有直接映射的便利，需要记录下对应的ppn以用于释放。
另一个需要注意的是，在映射用户态的代码的时候，他可能是需要一个巨大的连续的页面的，而并非像内核态那样只能接受2M及以下的连续页面（因为是直接映射）
因此必然面对的问题是，在释放页面的时候，会不得不释放多个entry。
而这时候，我必须考虑free pages的语义问题。

同时需要把root space也作为参数传递进去
同时需要在nexus中增加root space的字段，用于区分用户态的不同的地址空间

# 增加的设计2 allocator id
按照前面的设计逻辑，在内核中，我们存在多个allocator
当某个allocator（通常是某个CPU上的对应的allocator）试图释放某一块内存，但是发现id不是自己，他就必须去寻找allocaotor是对应的ID的那个allocator，用它去释放。
但是问题来了。
我如何根据id获取对应的allocator？

一个简单的思路是，使用一个array
array里面存放多个allocator的指针。
然后对应的id就是array的index

这当然是一个自然而然的想法，寻找速度肯定是最快的。
但这意味着，我需要开一个跟cpu大小相适应的array。
另一个方案是链表查询。
当然其他数据结构也都是可以做的。唯一的问题是，本来allocator就得应付多核的情况，这可能包含非常多个核心。
而链表查询的效率在核心数量多的时候显然是不高的。
我们这里最多期望存在多少个核心呢？
128个
因为map handler需要占用页表项，每次占用4个，最多存在512个页表项，因此只能占用128个map_handler
对应的，也只能有128个allocator

所以，直接开个数组就完事了。
当然，要记得，在对应的数组里面，初始化的时候写入指针哦。
# 增加的设计3 nexus的多核问题 
现在 ，nexus肩负着内核空间和用户空间的页分配器的作用。
对于内核而言，多核问题并不是问题。理由很简单，会在内核调用alloc pages的地方可能很多，而这个分配，一定是对应的每个cpu去单独分配的，这点没有问题，也就是说，这时候，每个cpu占用各自的一部分的页面。（由于他们之间使用锁去抢页帧分配器，所以一定是可以的。）而有问题的是什么呢，是在释放的时候，另一个cpu使用他的nexus去释放属于我的页面，这肯定会出问题——找不到。可是，在内核中会调用的这个free_pages函数的地方在哪里呢，一个是spmalloc的地方，但是spmalloc本身处理了多核的问题（设计为多个分配器如果释放的时候发现不是自己的，需要调用别人的对应的nexus去释放。）所以这没问题。
另外就是内核直接试图分配一批页面，随后释放的地方了。但是内核理论上不应该在这里进行分配若干个页面这种事情（除非使用spmalloc），因为这种数据结构确实太大了。我们可以认为，内核要么不使用超过2048大小的内存块并使用spmalloc管理，要么使用若干个页面大小的内容，并永久不释放，要么使用并且试图释放，并且额外记住他的nexus_id，并如果不是当前核心的nexus，需要加锁。
但是对于用户态，这件事情就不一样了。

spmalloc并不用于用户态，用户态也不应当使用spmalloc，用户态只能使用page分配器nexus
nexus也不应该试图释放另一个nexus中的内容。除非将整个用户态进程的内容都放到另一个nexus上

所以难点在于用户线程的迁移。
现在一个用户线程从一个cpu迁移到另一个cpu了，如果试图释放，这时候就有问题了。
我认为对此的解决办法是，因为每个用户态部分的分配的页面的内容，都一定在某个nexus的管理之下，所以迁移线程的时候，需要把他的entry从原先的nexus挪到新的nexus中去。

因此需要额外增加一个nexus的操作，迁移
由于原先已经实现过get free entry和free entry的调用，现在相比之下要简单很多
简单来说，只需要锁定，然后在新的nexus中获得一个新的entry，并复制过去，同时调整这个entry在新的nexus中的红黑树

相应的，nexus中需要额外的数据结构来记录这些东西，以用于快速的在不同nexus之间迁移。
当然，一个更简单的方式是，采用双键值的插入和查找。
也就是说，以vspace root（物理根页面）为第一键值，address为第二键值
当然，这同样意味着需要设计额外的查找接口。

另外需要额外实现一个只有vspace root查找的接口。由于按照第一键值排序，所以所有相同的vspace root的都应当放在一起，这个接口会返回按照该vspace root给出的第一个节点。
随后，按照这个接口逐步访问下一个nexus节点。

# 增加的设计4 nexus的多地址空间
迫不得已，又要增加设计了。
具体来说，当开启多个进程之后，原先的设计是，所有的页面申请都记录在一棵红黑树下。这会导致多个地址空间的问题。
为了解决这个问题，在前面的设计中，已经描述了在space root使用键值排序的方法。

但是，在分配的页面够多的时候，这并不是什么好方法，会造成红黑树大小的急剧膨胀。

所以这里重新设计为，使用两个rb node。一个用于查找对应的红黑树。另一个用于查找对应的在树中的地址。

对应的接口也需要重新改写。

同时，需要特判kernel的情况。对于kernel的情况，因为他本身是非常容易被访问到的，并且也是公共的，因此，我们设计为挂载在根上。

# 增加的设计5 用户态的地址空间和地址空间范围的问题（TODO）
一个是lazy alloc，也就是跟一个虚拟页面并没有影射到具体的物理地址上相关（不过这个一般用在用户态，内核态可以不管，因为，内核态的分配策略就是，直接分配并映射，它必须有）。在分配时需要指明是否需要分配物理空间，释放时如果没有对应的物理页，那就不管它。

另一个是改写map handler的flag相关，比如，如果可以override写入，那么就允许，否则才出错。（不然还得unmap一下再map，当然，对应的，它不应该丢失掉原有的物理页，所以需要返回是否存在override的情况以及对应的原先的物理页是谁的问题。

还有一个巨大的问题是，为什么要在nexus里维护这个映射关系，因为这个映射关系已经在页表中，我觉得这部分可以重构了。改成维护范围，而不是，维护直接的映射，因为映射关系已经有了。
但是如果直接从页表中查找，会面临另一个问题，那就是，它可能有很大的tlb flush开销。需要注意这一点

另一个需要改写的地方是，检查pte的有效性，并不是仅仅看是否为0，因为，还有可能无效，但是有些地方可能需要为0，需要仔细斟酌。

## 先说那个区间映射的事情
没必要维护上面的映射关系，但是需要像Linux一样维护范围。

先考虑一个事情，范围维护还在后面，就是nexus entry分配的事情。目前每次开一个新的vspace并没有新建一个对应的nexus page，而是还是依然从当前整个vspace树上的nexus进行entry的分配，这在多个vspace的时候分配entry会相互影响。这不合适。所以先改造成为，每开一个vspace，新整一个开始的page。这虽然会增大内存的使用，但是会减少vspace之间的冲突。同时在用户态的page的管理上，就必须从vspace的根节点里面去分配而不再是从kernel的那个nexus root开始分配。

当然，这也需要记得释放整个vspace对应的nexus entry那些对应的页面。（这部分应该没有什么share之类的，所以就按照独占释放就行了。）

然后来看现有整个映射的逻辑，整个get free page和free page的逻辑其实也是分为用户态和内核态两种，所以按照kernel和user两个分类来说，还是有一定差别的。

kernel：

get free pages是这样的：先从kernel的nexus中获取一个entry。然后找buddy分配物理页面，并根据情况选择2M或者多个4K的映射关系。
（但是问题不少：首先，目前是是找buddy分配的，但是可能存在多个底层的物理内存分配器。buddy在我的设计里面最多能够分配2M，不过在别的zone中，有些是可能分配更多的物理内存的，尽管这部分还没写。但是，不能在这里检查分配大小是否大于2M。应该根据物理内存分配器的返回来判断是否成功。另外，目前，这里只支持一次2M的分配，可能之后需要统一的分配吧。）

free pages的逻辑是这样的：和get free page相反，先去找到对应的节点，再释放对应的位置的映射。再释放物理页面。最后释放在vspace中的链表，和他在vspace中的rb node，最后释放这个nexus entry

（这也问题很大，但是后面可以再说咋改）

user：

get free pages是这样的： 这个逻辑整体大的很简单，就是先查找是否存在，如果不存在就插入，先分配2M的，再分配4K的，就这么简单。在分配2M的过程中的逻辑也是一样的，先找个entry，然后再找物理页面分配器要一个页面，然后map。最后塞入对应的nexus entry

free page就是对应过程的反过程。

对应的逻辑现在需要进行修改：

kernel：

get free pages：首先处理区间，但是这个事情确实没有那么简单，因为实际的做法是按照加上一个偏移的方式映射到对应的内存的。所以实际上，得先分配物理页面（这里得找对应的zone，上层需要处理给定zone的逻辑嘛？还是说在这里根据情况自动找？我打算让上层处理了）。然后之后，找到了物理页面。自然计算出来了需要插入的内存区域。

首先检查内存区域是否跟已有的内存区域相邻，相邻且属性一致就进行合并等操作（但是相交等问题则需要报错，因为按道理内核里面不应该有这个相交的区域）。如果不相邻，则插入新的nexus entry到系统中去。

然后再说map的问题。map的问题是要处理2M的大页的问题。那我们只能在区间属性上，增加一个2mpage的属性并和普通的4K页面当成两种，如果不相同，那就新塞一个nexus entry，并且不允许简单的合并（也就是发现一堆连续的4K页面拼起来刚好一个2M页面就合并的操作。不然，如果连续的4K页刚好凑够2m还需要反复检查并修改映射，挺麻烦的。况且既然分配的时候这些4K页并不是连续分配，那么可能其中一部分相当固定，而另一部分频繁变动，如果频繁变动的部分反复get free pages并free pages，这样来回合并，拆分，是很讨厌的。），除非手动指定。所以其实在上面插入nexus entry区间的逻辑中，本身需要加入如果这个区间超过2M，就得插入新的nexus entry的逻辑。

这里还有个问题，我咋知道我到底让物理内存分配器分配了多少内存？？？在buddy里面其实是向上对2的幂取整的。但是其他分配器也不好说。哎。改内存分配器吧，里面加个参数说明分配了多少吧。。。

free pages：free pages的含义，那很自然的，一样首先查找并检查试图删掉的内存区域是否跟已有的内存区域相邻等一堆检查。如果在头尾，且小于就做一些缩减区间等等操作。相等就直接删除节点，如果在中间，还需要考虑分裂成为两个等等操作。这里有个问题就是，如果按照上面的拆分成为一个2M和4K的拼接体，可能释放的时候，跨越多个nexus entry，这必须处理。然后再取消映射，同样按照上述的逻辑去释放。

同样的再说2M大页的问题，我们直接了当的设定，在某次释放中，不允许对2M页面的区间，做释放中间部分4K页面的操作（正如我们不允许上面的合并一样），无论是以释放区间但保留2M页面的方式，或者拆分成为多个4K页面的方式。所以在上面的解决区间的问题上，因为释放也有个page number参数嘛，我们不允许在上面的过程中有这种操作。

user：

get free pages：用户态的区别就是，我不需要再进行偏移的映射，因此，这个维护区间的操作是更为清楚明白的，也就是说，我直接加区间就行了。然后再第二步去找物理内存分配器分配并映射。那当然啦，他可能涉及很多个区间，也有2M和4K的差别，所以需要注意他的逻辑。

另一个地方在于，在用户态的内存分配器一定是找buddy分配的（都不追求物理内存分配器的连续性了，那要什么其他分配器嘛，那个连续的分配器也不一定够用对吧）

free pages：释放逻辑也差不多，先试图释放区间，再取消映射，这是个很自然的逻辑。

除此之外需要增加（或者修改）的接口：

一个是既然要改成区间的方式，那么get free page的语义本来就应该是，既要nexus中分配区间，又要进行映射，但是，其实可以增加两个，一个是增加和取消区间，一个是在已有的区间进行映射和取消映射。但是正如前面说kernel的内容的问题，他在内核需要分配内存本身需要的就是先分配再映射，这还得内核和用户态分别考虑。我们不期望内核态会有缺页，那缺页一定是内核没检查，差评。

然后考虑可以复用的代码，首先维护一个区间的操作，是user和kernel都需要的内容。这部分代码可以复用。关于分配并映射。这俩可能还真的得单独维护。


## 锁的事情

最开始当然是一把大锁保平安，但是并非总是如此，嗯，怎么说呢，维护vspace这个红黑树，本身需要一个锁，而维护每个vspace内部的红黑树，也需要一个锁。这是可以接受的。所以实际上需要两把锁。

## 反向映射问题

这也是一个新增的设计。

首先，先说问题。当我试图swap出去一个页面的时候，这时候，我会从物理页面的视角去查看我的页面，并逐出一个页面。但是对应的，需要在该物理页面对应的pte上，标记它为invalid（也许可能需要遍历并修改pte中的内容指向某个磁盘块的位置。反正就是修改吧）

同样的，COW也有这样的问题。

所以，此时需要增加，某个page对应的映射集合的关系。

除此之外还存在内存分配的问题。我们在这里无法动态分配内存，它必须使用有限的静态方案。

所以我需要对每个物理页面维护一个mapping的数据字段，而buddy则是建立在这之上的。它们可以静态的链接到所有使用了它的nexus entry。

这样的话，index可以只要40-12=28位最少（前面设计了最大可以支持的物理地址空间）。需要一个32位的引用计数，那么剩下4位用于存放目前的flag，我觉得是很足够的。嗯，就这样吧。省下来了一个mapping的指针位置。

除此之外，也需要对nexus的定义做一些额外的修改以便于静态分配。

简单来说，我们定义了，一个被多个地址空间使用的nexus区域，只能按照4K（如果允许1M分页，那么也可以1M）去进行划分，从而便于链接。他和其他非共享的nexus entry在试图共享的时候，必须进行拆分当成不同的区域来对待。

page frame的mapping字段指向了该nexus entry，（但是nexus entry没必要再维护指向page frame的映射），并在nexus entry中通过双向链表链接了其他的nexus entry。

nexus需要为此提供额外的接口，而不能让底下的pmm去处理refcount的事情（当然，这需要底下的pmm提供一些修改接口）

另外，这也影响上面的get free page和free page的实现。如果get free page，需要查看对应的mapping信息。如果没有，则需要设置该信息为链接的情况，如果有，则需要加入链表。

最后，需要额外考虑非buddy的物理内存分配器情况下的实现，因为这里只有buddy的数据记录了mapping的情况。通常来说，我们只允许kernel中使用其他的物理内存分配器中的内容（比如dma等区域）。而在用户态的这些内容，才会当成一个一个4K page去做并允许shared。（总之这里需要一些谨慎的处理）而且这个mapping这里，涉及到pmm和nexus两层，需要谨慎对待。