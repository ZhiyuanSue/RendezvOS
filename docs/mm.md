# pmm
首先说pmm模块的管理，pmm管理说到底就是管理页框
这部分，首先需要从multiboot获取所有可用物理内存的位置，并且减去内核占用调的部分。

随后我打算分成多个块，使用一个链表串联起来这些块（由于本身存在对齐的问题，并且linux也是分块进行内存分配的）

另外，需要使用buddy算法来分配物理页框，管理具体某一块的内容。

# buddy实现
首先参考Linux的实现中，他将内存划分为几个部分，称为zone
每个部分有个水位线
然后每个部分在这里面buddy会分桶，每个桶是2的倍数，针对不同的桶，去做一个链表。
在这里的设计，我们直接简单粗暴的对每个桶使用一个大的page_frame数组，分别表示每个桶里面的所有page。
事实上，静态开销相比之下更大一些。但是因为是数组，不需要动态分配，可以使用，也有其优势。

对于alloc
最开始应该是只有最高阶的桶会存在链表，如果要分配1个page，那么会从下向上查找，找到链表头的一个page_frame，先分成两部分，对应的page_frame结构是非常好查找的。把空的那块插入链表，不空的那块继续分配。

分配完了之后，如果这个页面已经有被分配了的，需要置位have alloc。

如果一整个大页面都分配了，需要递归的置位。

对于free，其实应该设计两个函数，第一个是free单个page，第二个是free多个page

对于单个page，首先检查是否分配出去了，如果没分配出去，那滚蛋，错误的free

然后释放，挂链表，如果兄弟也释放了，那么摘下兄弟，并把父亲给挂上。

对于多个page，他未必是按照order的内容去分配的，但是必须是连续的。
举个简单的例子

两层

 [0,1]   [2,3]   [4,5]
[0] [1] [2] [3] [4] [5]

全都是满的
现在要释放1，2，3，4，四个页面。
这里面，从1开始，1最后一位是1，所以他只能释放1个页面
然后现在最低变为2，2小于等于4，所以他最多只能释放2个页面，所以需要把 [2,3] 和 [2] [3] 都释放掉。
然后是4，b100，所以最多可以释放4个页面，但是，因为4+4> 4，所以只能到最终端，就是4,4的区间释放掉4，只有一个页面。
如果有多层的话，2向上溯只能上溯一层，但是4可以向上溯2层，如果判断出来需要释放4,8，那还需要继续上溯。
直到顶层页面。

相比于Linux的bitmap的异或，这未必是最优的，但是是可行的，而且实现难度更低一些。

在整个buddy pmm结构体中，使用多个bucket数组表示所有的内存之后

考虑zone，每个zone当中，主要使用两个指针数组来表示，一个指针数组用于表示在不同的order中，这个zone所指向的第一个page frame，在这个指针之前的page frame则是属于前一个zone，在这个frame地址之后的所有frame直到下一个zone的第一个frame，包括其中的可用的不可用的frame。

另一个指针数组则是用于描述可用的链表。这些frame则不是连续的，而是使用链表来连接的，也就是上面说的那个buddy的数组了。

# pmm内存的layout(x86_64)

high address
	-- - -- - -- - -- -
	buddy static part
	-- - -- - -- - -- - _end ROUND_UP 4K
	empty
	-- - -- - -- - -- -	_end
	kernel
	-- - -- - -- - -- - 0x100000
	bios part	:we cannot use
	-- - -- - -- - -- - 0x0
low address

# 这种静态buddy算法能够支持的物理空间
在这里，我们允许每个bucket的位置都可以分离而不是所有的bucket都必须连续的进行分配
除此之外，原先我的设计，将内核放置在最后1G的空间中。
但是考虑到极大的内存分配过程。这点空间是不够的，
另外，原先这是一个todo，但是现在被废弃了，大概是这样的，考虑到系统可能使用多个memory区间，然后需要存放这些page frame的情况，如果内存需要占用的空间非常大，pageframe 的一对array并不总是能够放到跟内核连续的这个region里面
一个简单的想法是，我换个区间去放，另外，我多个pageframe的array不一定需要连续的存放。
但是，我放弃了这种修改，原因是它需要不定的去增加新的1G页，这也极大的增加了我的工作量。
目前，我的pageframe必须存放在和kernel相同的1G空间内。

如果不考虑这种限制。
此时，对于能够支持的物理空间的限制，仅仅只有在page_frame中的next和prev的位数的限制，我这边设置28位是因为在qemu的x86_64中使用了40位的物理内存空间（大概1T）

因此为了尽可能的通用，在aarch64的设计中，我也将tcr_el1的ips位设为最大40，防止出事情。

# shared memory 的处理。
对于每个page frame，还设置了一个字段表示shared memory的count数量，我认为6位表示count==64是完全足够的
不过，现在并没有加上这部分的处理逻辑，因此，这部分留作TODO


# zone的avaliable list的问题

在原有的设计下，他仍然可以正常工作，但是在新的设计下，所有的index都被有需要的时候用于表示物理内存了，所以，因此，我只能选择不使用头结点的方式

在这种方式下，在插入的时候需要判断这个指针是否为空，如果为空直接赋值avaliable_zone_head[index]，否则则调用frame list add 系列函数

同时，在删除的时候，需要判断是否只剩下这一个节点了，如果是，则将avaliable_zone_head[index]置为null，否则则正常调用frame list del

不仅仅如此，我在实践过程中发现，还需要在del head的时候更新链表的头节点指针，以及在add head的时候也需要更新（注意更新头节点指针和做删除以及add的顺序）

# 虚拟内存的处理
最开始我以为需要vma结构体进行管理，并实现了红黑树，但是，后来我意识到，只有在用户使用的空间中，需要进行如此管理，而在内核使用的的空间中，则无需如此。这是个恒等映射即可。

所以实际上，需要向pmm要一块空间，并随即映射到高地址区域。通过slub进行管理即可，虽然其实也可以用红黑树就是了。
对于vma管理结构，虽然很经典，也是我经常遇到的东西，不过，我觉得我在之前缺少实现的经验，所以会错误的把内核这部分也用vma进行管理。


# 内存分配器
首先是描述如何从内核获取页面
我们使用恒等映射的方式去做，所以，当我们从pmm获取一个或者若干个页面的时候，直接映射过去即可。
除此之外，正如上面说的那样，这部分内核的管理不能依赖于vma的管理，而是让内核内存分配器来完成
关于内存分配器的管理。我决定自己设计（实在是那个pmm的page结构，没法拿来复用。。。比如slob就直接用union‘寄生’到这个page结构体中，但我不能这样做）
至于slub，无论如何还是过于复杂了。

对于实现的设计如下
实现为按照8/16/24/32/48/64/96/128/256/512/1024/2048这几个大小

slub的cpu和numa node两层对于os级是可以考虑的。（说到底，我只有一个cpu和一个numa node，不过为了后续可扩展性，这部分设计需要预留出来，cpu将来大概率会扩展成多核）
对于cpu的list的访问，可以不加锁
而对于numa node需要加锁访问。

如果上述都失败了，才会去找pmm继续分配，然后映射过去。

## 实现

在实际中，我使用了如下的实现

从上到下，最上层
若干个allocator，他们依赖于一个get_free_page模块，互斥的从内核获取直接映射的块
page的管理模块

每个allocator下面包含12个group，每个组里面的object payload的大小是8/16/24/32/48/64/96/128/256/512/1024/2048

每个group里面包含两个链表
一个是部分使用的链表，一个是空链表
空链表的作用是，如果其他group不够用了，那么优先从这些group的空链表上供给
只有其他组的空链表都不够用了，才统一从系统为每个group的空链表分配一次chunk

每个链表由若干个chunk串联起来，一个chunk被设计为包含2个page。
在chunk头部有一个头，然后后面跟一定大小的padding，之后密集排列objects

object内部有个header，以及alloc实际返回的addr的payload。
所以每个object实际的大小分别为
24/32/40/48/64/80/112/144/272/528/1040/2064
之所以这么设计
1、统一是8192，所以free的时候根据地址很快的发现位于chunk头部的header管理结构
2、因为包含了头部以及每个object的头部，所以对于2048大小的块要紧密排列是困难的。如果只有一个page，那么浪费太严重了。
3、还必须考虑释放的问题，如果一个chunk里面有非常多的pages，那么很可能哪怕这个块只有1个使用了的object，仍然难以释放，chunk占用的page越多，这种浪费越严重。
所以综合考虑，我认为一个chunk 2page是合适的。


除此之外还需要考虑的是分配和释放的时候，对于一整页的处理
因为在分配的时候，如果是4096的整数倍，我们会试图直接调用get free page去做。在内核中，需要非常大的缓冲buffer的时候，是可能发生的。但是通常来说，这不应该超过2M（否则buddy难以处理）

这种类型的指针非常好识别，他一定是4K对齐的，而在allocator内部，在第一个page中，4K对齐的地址是header的地方，而第二个page中，可以计算出来，上述的object payload也不会对齐到4K的位置。

所以，可以通过这种方式非常简单的判定，是不是整个页面进行的分配。
但是如果试图释放，我们需要知道多少个page在里面。
同样的道理，对于allocator自己本身使用的page，也需要管理以便于释放

因此，还需要在所有allocator之外，增加一个所有page的虚拟页的管理器。
这个管理器我决定使用红黑树来实现。

===


具体的实现是这样的
我试图让他bootstrap

然后结构上，每个节点包括一个红黑树节点，一个free list，起始地址和size，size应该是表示分配的页面号的，使用u32表示size即可，如果表示终止地址需要u64，浪费
最后多一个管理管理节点页面相关的变量，u32，表示这个管理节点所管理的页面是存储管理节点的，并且其值表示该页面剩余的可用管理节点数目，用于等于零的时候去释放该管理页面。

用于管理的节点，以一个array的方式，排列在一个内存页面中

当试图申请一个页面的时候，首先检查申请的页面数量，超过2M，就不用分配了（我们默认内核不会干这种事情，内核的数据结构不应该这么大）
这里有一个manage free list，在这个链表中，存放了有空闲entry的管理页面的链接，在释放的时候，会通过红黑树去找，如果释放之后，查看这个manage free list node，发现指向自己，说明他现在不在整个free list中（因为头在根节点中），此时他释放了一个entry，需要通过根节点的header，插入。
如果插入之后，发现整个链表用完了所有的空间，就从这个free list上摘出去。
初始化的时候，需要先初始化根节点，再把它放到根节点指向的链上去。

查看free list是否可以分配一个管理节点
如果可以分配，那么从free list上分配该管理节点，申请相应的物理页帧并恒等映射，同时修改管理节点的起始地址和结束地址
找到该页面的管理节点（也就是该页面的第一个节点），将其剩余可用管理节点数量减一。
同时插入红黑树

如果free list不能分配一个节点，说明他已经在使用的所有管理页面中没有足够的空间了，需要新分配一个管理页面。这时候，可以先查看后备管理页面
然后，这时候，从页帧分配器中要一个页面并恒等映射。将该页面进行分割，第一个节点被当做该管理页面的管理节点（也就是自举）
执行插入红黑树，同时指定起始地址和size。
同时，对其进行初始化，将剩下的(pagesize-管理节点的size)的空间进行分割成一个个的管理节点，并插入free list链表（即初始化该空管理页）

当试图释放一个页面（或者连续的若干个页面）的时候，从红黑树中查找该页面，找到该页面的管理节点。根据该管理节点，释放该页面给pmm，并取消映射
同时将该节点从红黑树中删除，并插入free list，该管理节点的起始地址和size进行清理。该页面的管理节点的可用管理节点数目减一。

当减为0的时候，查看根节点（一会描述）是否包含有后备页面指针，如果没有，存入后备管理页面，防止抖动问题（也就是这里刚释放完，立马试图再分配一个，反复释放分配，会造成抖动。）如果存在后备管理页面，则释放本管理页面。

根节点（需要一个根节点来描述整个红黑树，这是由于红黑树本身结构导致的），为了自举，我们保证这个根节点管理结构和管理节点相比，内存占用不得大于管理节点。从而可以替换掉他（就是用个union，完事）
所以在初始化的时候，我们分配一个管理页面，0号节点用于放置管理节点，然后1号节点实质上是个根节点。
将根节点的指针指向0号节点的指针
同时根节点有个指向后备页面指针的东西，防止抖动。
还需要有个指向pmm的指针。

注意！！！

我们试图从pmm进行分配，pmm其实本身包含很多的zone。这里面需要加锁。TODO

pmm不仅仅只用于内核的数据结构，还用在比如说用户的内存的分配（实际上是page fault处理的），所以，请务必分清楚哪些代码是pmm需要运行的，哪些代码是内核数据结构的。

同时，我们需要提供get_free_page和free_pages的接口，这里面参数需要指定使用哪一个zone，等等细节。总而言之，我认为我描述清楚了这个页面管理的算法。

对于名称，我决定取个名字叫Nexus，嗯，就叫这个了。

# 页表的管理
对于页表的处理，我同样遇到了先有鸡还是先有蛋的问题
只考虑最简单的恒等映射的情况
当我试图从页帧分配器拿一个新的页面的时候，可能需要创建新的页表中的页面（可能存在于多级页表中都触发这种创建）

然后这时候，我需要继续的为了这个页面去创建新的页表映射。而一个设计不合理的页表映射到虚拟地址的方案，可能会造成多次的这种创建。
从而难以下手。

我的解决方案

首先，对于页表，我只是需要在修改他的时候能够访问这个内存，并进行修改即可，但是没说在正常转换的时候仍然需要这个内存。
因此，我有理由认为。我可以在结束后，取消这个映射。

所以每次更新页表，我只需要做最多四次这种映射，然后修改对应位即可。如果缺失，则只需要分配一个新的页，只做映射到固定的四个页面上。修改对应的位也就完事了。
也就是说，在虚拟空间中，不保留所有映射的内存，只有需要访问的时候，一遍一遍的映射即可。

我原本试图使用页表自映射机制，但是正常情况下内核不应该使用超过512GB的空间，不过，谁让我们的设计给了1T呢，需要额外考虑代码，就相对复杂，如果后期需要，可以更改。

另外，这几个用于映射的临时页面的地址，在实际中，是可以认为是整个系统运行期间都是固定的，所以可以简单的计算变成全局变量来指明对应的页表项的指针。当然，也可以不用四个页面，只用一个页面，更加节省空间。但是我认为这需要多次tlb flush,反而造成性能损失。因此不做如此设计。

关于全局变量分配这个页面还是分配在栈上面。
我倾向于全局变量，因为可以预先计算出来所有的index，而栈上的好处是减少对应的攻击可能性，相应的，每次重新计算，需要额外的计算开销。

# 总结
以上这些，我也算是一边编写代码，一边加以设计的。所以如果存在和上述描述冲突的细节，请务必以代码为准，因为代码可以跑通。
总体而言，还是主要以分层思想为主
最底下是一个buddy pmm
这个是所有的核心应当且必须公用的
在这之上，每个核心都应当有一个map handler映射器，用于辅助映射
基于这个映射器，实现了每个核心都有的nexus，包含了对于可能的虚拟页面的获取和释放。
基于nexus，再实现了一个spmalloc的一个内核对象分配器。
正常来说，在内核中，由于我尽量去获取2M的大页，其实分配器需要锁定buddy pmm的情形很少。所以实际上会留给pmm的锁竞争压力并不大。

目前还没考虑到swap的问题。正常而言，如果从buddy获取不到页面，需要进行置换。
但是这一步应当在nexus的错误处理步骤中进行处理。也就是说，如果从nexus获取不到页面，他在这里会进行swap，以供内核存在足够的页面进行操作。

# 增加的设计1
额，迫不得已，又要新加设计了。
简单来说，是关于nexus是否能够成为全局分配器的问题。
首先，对于内核态和用户态而言，这两者显然不能混为一谈。在内核态，由于集中统一直接映射，这不会存在问题。但是，在用户态，则不是这样。
因为本身记录的就是虚拟地址，而且其映射关系是混乱的，因此，可能在不同的地址空间用同一个虚拟地址。这怎么可以呢

所以，事实上，还需要额外考虑内核态和用户态的差别。
考虑到aarch64使用两个页表的情况。我觉得，这是可行的。
也就是说，所有地址空间的内核态公用一个nexus分配页面，然后每个用户态各自分配用一个nexus分配页面。
但是也正因为如此，还需要判定用户态的地址，（因为需要放在TTBR0那里）
这时候，就得根据传入地址的位置了，如果是开头17位全都是1的地址，都被认为是内核的地址（按照canonical格式，前16位是符号扩展，所以还得加一位符号位才行），使用内核的分配器。否则使用用户的分配器。
事实上，我们通常定义KERNEL_VIRT_OFFSET作为内核情况下的参数。

而内核分配器则是每个cpu一个，用户的分配器则是每个进程一个。
但是用户的分配器又有一些特殊。
在nexus中，涉及到两种类型的页面，一种是管理页面，一种才是分配出去的页面。
管理页面必须放置在内核空间中，否则可能被用户本身所破坏掉。
其他的页面都被放置在一个用户地址空间中。

为此，需要改动的为以下部分
map函数中需要增加自选的falgas
以及在get free pages以及free pages的逻辑中，增加对于用户态页面的判定。
还有就是映射页面的ppn，在用户态，并没有直接映射的便利，需要记录下对应的ppn以用于释放。
另一个需要注意的是，在映射用户态的代码的时候，他可能是需要一个巨大的连续的页面的，而并非像内核态那样只能接受2M及以下的连续页面（因为是直接映射）
因此必然面对的问题是，在释放页面的时候，会不得不释放多个entry。
而这时候，我必须考虑free pages的语义问题。

# 增加的设计2 allocator id
按照前面的设计逻辑，在内核中，我们存在多个allocator
当某个allocator（通常是某个CPU上的对应的allocator）试图释放某一块内存，但是发现id不是自己，他就必须去寻找allocaotor是对应的ID的那个allocator，用它去释放。
但是问题来了。
我如何根据id获取对应的allocator？

一个简单的思路是，使用一个array
array里面存放多个allocator的指针。
然后对应的id就是array的index

这当然是一个自然而然的想法，寻找速度肯定是最快的。
但这意味着，我需要开一个跟cpu大小相适应的array。
另一个方案是链表查询。
当然其他数据结构也都是可以做的。唯一的问题是，本来allocator就得应付多核的情况，这可能包含非常多个核心。
而链表查询的效率在核心数量多的时候显然是不高的。
我们这里最多期望存在多少个核心呢？
128个
因为map handler需要占用页表项，每次占用4个，最多存在512个页表项，因此只能占用128个map_handler
对应的，也只能有128个allocator

所以，直接开个数组就完事了。
当然，要记得，在对应的数组里面，初始化的时候写入指针哦。